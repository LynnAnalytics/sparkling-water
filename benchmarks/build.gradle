description = "Sparkling Water Benchmarks"

apply from: "$rootDir/gradle/utils.gradle"
apply plugin: 'java-library'

configurations {
  sparklingWaterAssemblyJar
}

dependencies {
  sparklingWaterAssemblyJar(project(path: ':sparkling-water-assembly', configuration: 'shadow'))

  api(project(":sparkling-water-ml"))
  api(project(":sparkling-water-core"))

  compileOnly("org.scala-lang:scala-library:${scalaVersion}")
  compileOnly("org.apache.spark:spark-sql_${scalaBaseVersion}:${sparkVersion}")
  compileOnly("org.apache.spark:spark-mllib_${scalaBaseVersion}:${sparkVersion}")
}

task cleanBenchmarkFiles(type: Delete) {
  delete "build/terraform"
  delete "build/local"
}

task copyBenchmarkFiles(dependsOn: cleanBenchmarkFiles) {
  doLast {
    copy {
      from 'src/main/terraform'
      include "**/*.tf"
      into "build/terraform"
    }
    copy {
      from '../templates/src/terraform/aws/modules'
      include "emr_security/*.tf"
      into "build/terraform/aws/modules"
    }
    copy {
      from 'src/main/local'
      include "**/*"
      into "build/local"
    }
    copy {
      from 'data'
      include "**/*.csv"
      into "build/local/data"
    }
  }
}

task substituteScripts(dependsOn: copyBenchmarkFiles) {
  doLast {
    String baseDir = project.buildDir.toString()
    String tfBaseDir = "${baseDir}/terraform/aws/"
    String localBaseDir = "${baseDir}/local/"
    List<String> scripts = [
      "${tfBaseDir}/variables.tf",
      "${tfBaseDir}/modules/emr_benchmarks_deployment/variables.tf",
      "${tfBaseDir}/modules/emr_benchmarks_deployment/main.tf",
      "${localBaseDir}/variables.sh",
      "${localBaseDir}/docker-compose.yml"
    ]
    scripts.each { path ->
      String contents = file(path).getText('UTF-8')
      File packageFile = configurations.sparklingWaterAssemblyJar.singleFile
      String benchmarksFileName = "sparkling-water-benchmarks_$scalaBaseVersion-${version}.jar"
      contents = contents
        .replaceAll("SUBST_PACKAGE_FILE_NAME", "${packageFile.getName()}")
        .replaceAll("SUBST_PACKAGE_FILE", "${packageFile}")
        .replaceAll("SUBST_BENCHMARKS_FILE_NAME", benchmarksFileName)
        .replaceAll("SUBST_BENCHMARKS_FILE", "$buildDir/libs/$benchmarksFileName")
        .replaceAll("SUBST_H2O_VERSION_NAME", h2oMajorName)
        .replaceAll("SUBST_H2O_VERSION", h2oVersion)
        .replaceAll("SUBST_H2O_BUILD", h2oBuild)
        .replaceAll("SUBST_SW_VERSION", version.toString())
        .replaceAll("SUBST_SCALA_VERSION", scalaBaseVersion)
        .replaceAll("SUBST_SPARK_VERSION", sparkVersion)
        .replaceAll("SUBST_EMR_VERSION", supportedEmrVersion)

      file(path).write(contents, 'UTF-8')
    }
  }
}

task cleanOutput(type: Delete) {
  delete "output"
}

task runBenchmarks(dependsOn: [":sparkling-water-assembly:shadowJar", substituteScripts, cleanOutput]) {
  doLast {
    exec {
      def accessKey = project.property("aws_access_key")
      def secretKey = project.property("aws_secret_key")
      def publicKey = project.property("aws_ssh_public_key")

      environment("aws_access_key", accessKey)
      environment("aws_secret_key", secretKey)
      environment("aws_ssh_public_key", publicKey)

      if (project.hasProperty("aws_vpc")) {
        def vpc = project.property("aws_vpc")
        environment("aws_vpc", vpc)
      }
      if (project.hasProperty("aws_subnet")) {
        def subnet = project.property("aws_subnet")
        environment("aws_subnet", subnet)
      }
      environment("datasets", "datasets.json")

      commandLine "./run_benchmarks_aws.sh"
    }
  }
}

task runBigDataSparkToH2OConversionBenchmarksOnAws(dependsOn: [":sparkling-water-assembly:shadowJar", substituteScripts, cleanOutput]) {
  doLast {
    exec {
      def accessKey = project.property("aws_access_key")
      def secretKey = project.property("aws_secret_key")
      def publicKey = project.property("aws_ssh_public_key")

      environment("aws_access_key", accessKey)
      environment("aws_secret_key", secretKey)
      environment("aws_ssh_public_key", publicKey)

      if (project.hasProperty("aws_vpc")) {
        def vpc = project.property("aws_vpc")
        environment("aws_vpc", vpc)
      }
      if (project.hasProperty("aws_subnet")) {
        def subnet = project.property("aws_subnet")
        environment("aws_subnet", subnet)
      }
      environment("aws_instance_type", "m5.4xlarge")
      environment("aws_core_instance_count", "10")
      environment("datasets", "bigDatasets.json")
      environment("other_arguments", "-b DataFrameToH2OFrameConversionBenchmark")
      environment("driver_memory_gb", "8")
      environment("executor_memory_gb", "32")
      environment("run_yarn_internal", "false")
      environment("run_yarn_external", "true")
      environment("run_local_internal", "false")

      commandLine "./run_benchmarks_aws.sh"
    }
  }
}

task runBigDataH2OtoSparkConversionBenchmarksOnAws(dependsOn: [":sparkling-water-assembly:shadowJar", substituteScripts, cleanOutput]) {
  doLast {
    exec {
      def accessKey = project.property("aws_access_key")
      def secretKey = project.property("aws_secret_key")
      def publicKey = project.property("aws_ssh_public_key")

      environment("aws_access_key", accessKey)
      environment("aws_secret_key", secretKey)
      environment("aws_ssh_public_key", publicKey)

      if (project.hasProperty("aws_vpc")) {
        def vpc = project.property("aws_vpc")
        environment("aws_vpc", vpc)
      }
      if (project.hasProperty("aws_subnet")) {
        def subnet = project.property("aws_subnet")
        environment("aws_subnet", subnet)
      }
      environment("aws_instance_type", "m5.4xlarge")
      environment("aws_core_instance_count", "10")
      environment("datasets", "bigDatasets.json")
      environment("other_arguments", "-b H2OFrameToDataFrameConversionBenchmark")
      environment("driver_memory_gb", "8")
      environment("executor_memory_gb", "32")
      environment("run_yarn_internal", "false")
      environment("run_yarn_external", "true")
      environment("run_local_internal", "false")

      commandLine "./run_benchmarks_aws.sh"
    }
  }
}

substituteScripts.dependsOn build
